{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9f49a2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from secret_keys import openai_key\n",
    "import os\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "os.environ['OPENAI_API_KEY'] = openai_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064498aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# temperature param:- ratio of creative model\n",
    "                    # 1 is risky model but creative. Mostly used 0.6 or 0.9\n",
    "                    # 0 no risk\n",
    "llm = OpenAI(temperature=0.6)\n",
    "name = llm(\"I want to open restaturant for Indian food. Suggest a fancy name for this.\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b841955a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables= ['cuisine'],\n",
    "    template= \"I want to open restaturant for {cuisine} food. Suggest a fancy name for this.\"\n",
    "    )\n",
    "prompt_template_name.format(cuisine=\"Indian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ee3fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = OpenAI(temperature=0.6)\n",
    "\n",
    "# Chain 1\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables= ['cuisine'],\n",
    "    template= \"I want to open restaturant for {cuisine} food. Suggest a fancy name for this.\"\n",
    "    )\n",
    "name_chain = LLMChain(llm=llm, prompt=prompt_template_name)\n",
    "\n",
    "# Chain 2\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables= ['resto_name'],\n",
    "    template= \"\"\"Suggest some menu items for {resto_name}. Return it as a coma separated list\"\"\"\n",
    "    )\n",
    "food_items_chain = LLMChain(llm=llm, prompt=prompt_template_name)\n",
    "\n",
    "# RUN Chains\n",
    "#simple seq chain example\n",
    "\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "chain = SimpleSequentialChain(chains = [name_chain, food_items_chain])\n",
    "response = chain.run(\"Indian\")\n",
    "print(response)\n",
    "# remeber the response will of last chain only in this case response will be of food_items_chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673389f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we have the example of the seq chain which will give us the output of all the required chains\n",
    "\n",
    "# Chain 1\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables= ['cuisine'],\n",
    "    template= \"I want to open restaturant for {cuisine} food. Suggest a fancy name for this.\"\n",
    "    )\n",
    "name_chain = LLMChain(llm=llm, prompt=prompt_template_name, output_key=\"resto_name\")\n",
    "\n",
    "# Chain 2\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables= ['resto_name'],\n",
    "    template= \"\"\"Suggest some menu items for {resto_name}. \"\"\"\n",
    "    )\n",
    "food_items_chain = LLMChain(llm=llm, prompt=prompt_template_name, output_key=\"menu_items\")\n",
    "\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "chain = SequentialChain(\n",
    "    chains = [name_chain, food_items_chain],\n",
    "    input_variables = ['cuisine'],\n",
    "    output_variables = ['resto_name', 'menu_items']\n",
    ")\n",
    "\n",
    "chain(\n",
    "    {\n",
    "        'cuisine': 'Indian'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37c0be0",
   "metadata": {},
   "source": [
    "# AGENTS Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fb2894",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(temperature=0.6)\n",
    "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "agent.run(\"When was Elon Musk was born? What was his age in 2023?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5d4dc2",
   "metadata": {},
   "source": [
    "# SerpAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8c226825",
   "metadata": {},
   "outputs": [],
   "source": [
    "from secret_keys import serpapi_key\n",
    "from langchain.agents import Tool\n",
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "\n",
    "os.environ['SERPAPI_API_KEY'] = serpapi_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d6e281",
   "metadata": {},
   "source": [
    "#### New Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98656225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Way \n",
    "llm = OpenAI(temperature=0)\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(agent.run(\"What was the GDP of US in 2022 plus 5?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457ac01b",
   "metadata": {},
   "source": [
    "#### OLD WAY TO DO IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017754f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## OLD WAY TO DO IT\n",
    "\n",
    "# llm = OpenAI(temperature=0)\n",
    "\n",
    "# # the tool we'll give the agent access to. Note that the 'llm-math' tool uses an LLM, \n",
    "# tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "# #search = SerpAPIWrapper()  # uses os.environ[\"SERPAPI_API_KEY\"]\n",
    "# # tools = [\n",
    "# #     Tool(\n",
    "# #         name=\"Search\",\n",
    "# #         func=search.run,\n",
    "# #         description=\"useful for answering questions about current events\"\n",
    "# #     )\n",
    "# # ]\n",
    "\n",
    "# # Finally, let's initialize an agent with the tools, the langauage model, and the type \n",
    "# agent = initialize_agent(tools, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# agent.run(\"What was the GDP of US in 2022 plus 5?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4ba21c",
   "metadata": {},
   "source": [
    "# MEMORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ee456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template_name, memory=memory)\n",
    "name = chain.run(\"Mexican\")\n",
    "print(name)\n",
    "\n",
    "# expected output: Taco Fiesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4f5f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = chain.run(\"Indian\")\n",
    "print(name)\n",
    "\n",
    "# expected output: Maharaja's Palace Cuisine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb8b279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "chain.memory\n",
    "# conversation with alignment\n",
    "print(chain.memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493cc0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConversationBufferMemory this saves every ques and answer (combined 1 conversational exchange). This will send the all past history to OpenAI and OpenAI charges you per token.\n",
    "# so reduce the buffer size \n",
    "# example save only last 5 exchange using Conversation exchange "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20ac5b5",
   "metadata": {},
   "source": [
    "### Conversation Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bab9dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "convo = ConversationChain(llm=OpenAI(temperature=0.7))\n",
    "convo.prompt\n",
    "\n",
    "print(convo.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89f1bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "convo.run(\"who won first cricket world cup?\")\n",
    "\n",
    "#expected output: West Indies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79829c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "convo.run(\"5+5\")\n",
    "\n",
    "# Expected output: 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fa713e",
   "metadata": {},
   "outputs": [],
   "source": [
    "convo.run(\"who was the captain of the winning team?\")\n",
    "\n",
    "#expected output: captain name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da1a163",
   "metadata": {},
   "outputs": [],
   "source": [
    "convo.memory\n",
    "\n",
    "#expected output: All the past history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddf281d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "# k=1 sets means remeber only last 1 convertional exchange \n",
    "memory = ConversationBufferMemory(k=1)\n",
    "convo = ConversationChain(llm=OpenAI(temperature=0.7), memory=memory)\n",
    "\n",
    "convo.run(\"who won first cricket world cup?\")\n",
    "\n",
    "#expected output: West Indies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec03dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "convo.run(\"5+5\")\n",
    "\n",
    "# Expected output: 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd985143",
   "metadata": {},
   "outputs": [],
   "source": [
    "convo.run(\"who was the captain of the winning team?\")\n",
    "\n",
    "# expected output: I'm sorry, I don't know"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a924c34e",
   "metadata": {},
   "source": [
    "### As the conversationChain is deprecated following code is using RunnableWithMessageHistory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb7edd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import OpenAI \n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "# Initialize the model\n",
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "# Create the prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "# Connect prompt and model\n",
    "chain = prompt | llm\n",
    "\n",
    "# In-memory session history store\n",
    "session_histories = {}\n",
    "\n",
    "def get_history(session_id: str):\n",
    "    if session_id not in session_histories:\n",
    "        session_histories[session_id] = InMemoryChatMessageHistory()\n",
    "    return session_histories[session_id]\n",
    "\n",
    "# Wrap chain with message history handling\n",
    "runnable = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "\n",
    "# Run it\n",
    "response = runnable.invoke(\n",
    "    {\"input\": \"Who is Elon Musk?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"test-session\"}}\n",
    ")\n",
    "\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6606c9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
